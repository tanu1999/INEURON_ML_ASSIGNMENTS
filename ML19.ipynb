{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61112e43-eba1-454e-b33d-eef9f12fc9c7",
   "metadata": {},
   "source": [
    "1. A set of one-dimensional data points is given to you: 5, 10, 15, 20, 25, 30, 35. Assume that k = 2\n",
    "and that the first set of random centroid is 15, 32, and that the second set is 12, 30.\n",
    "a) Using the k-means method, create two clusters for each set of centroid described above.\n",
    "b) For each set of centroid values, calculate the SSE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fec4e1-ac0d-4a6d-a731-664e96877c60",
   "metadata": {},
   "source": [
    "a) Using the k-means method with k = 2 and the initial centroids provided:\n",
    "\n",
    "First set of centroids: (15, 32)\n",
    "- Iteration 1: Assign points to nearest centroid\n",
    "    - Cluster 1: {5, 10, 15, 20, 25}\n",
    "    - Cluster 2: {30, 35}\n",
    "- Calculate new centroids\n",
    "    - Cluster 1 centroid: (15 + 20 + 25) / 3 = 20\n",
    "    - Cluster 2 centroid: (30 + 35) / 2 = 32.5\n",
    "- Iteration 2: Assign points to nearest centroid\n",
    "    - Cluster 1: {5, 10, 15, 20, 25}\n",
    "    - Cluster 2: {30, 35}\n",
    "- Calculate new centroids\n",
    "    - Cluster 1 centroid: (5 + 10 + 15 + 20 + 25) / 5 = 15\n",
    "    - Cluster 2 centroid: (30 + 35) / 2 = 32.5\n",
    "- Clusters have converged\n",
    "\n",
    "Second set of centroids: (12, 30)\n",
    "- Iteration 1: Assign points to nearest centroid\n",
    "    - Cluster 1: {5, 10, 15, 20, 25}\n",
    "    - Cluster 2: {30, 35}\n",
    "- Calculate new centroids\n",
    "    - Cluster 1 centroid: (5 + 10 + 15 + 20 + 25) / 5 = 15\n",
    "    - Cluster 2 centroid: (30 + 35) / 2 = 32.5\n",
    "- Iteration 2: Assign points to nearest centroid\n",
    "    - Cluster 1: {5, 10, 15, 20, 25}\n",
    "    - Cluster 2: {30, 35}\n",
    "- Calculate new centroids\n",
    "    - Cluster 1 centroid: (5 + 10 + 15 + 20 + 25) / 5 = 15\n",
    "    - Cluster 2 centroid: (30 + 35) / 2 = 32.5\n",
    "- Clusters have converged\n",
    "\n",
    "b) For each set of centroids:\n",
    "\n",
    "First set of centroids: (15, 32)\n",
    "SSE = ((5-20)^2 + (10-20)^2 + (15-20)^2 + (20-20)^2 + (25-20)^2) + ((30-32.5)^2 + (35-32.5)^2)\n",
    "    = (225 + 100 + 25 + 0 + 25) + (6.25 + 6.25)\n",
    "    = 388.5\n",
    "\n",
    "Second set of centroids: (12, 30)\n",
    "SSE = ((5-15)^2 + (10-15)^2 + (15-15)^2 + (20-15)^2 + (25-15)^2) + ((30-30)^2 + (35-30)^2)\n",
    "    = (100 + 25 + 0 + 25 + 100) + (0 + 25)\n",
    "    = 275"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30edf7ff-a049-484e-b590-9b7873337667",
   "metadata": {},
   "source": [
    "2. Describe how the Market Basket Research makes use of association analysis concepts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003912f6-5e41-4670-a992-cf4845bab92d",
   "metadata": {},
   "source": [
    "Market Basket Analysis (MBA) is a data mining technique that is widely used in the retail industry to identify relationships between items that customers tend to purchase together. Association analysis is a key concept in MBA, which involves identifying frequent itemsets and generating rules based on these itemsets.\n",
    "\n",
    "The goal of MBA is to identify patterns of item co-occurrence in transactions, such as a customer buying bread and milk together. Association analysis is used to calculate support, confidence, and lift for itemsets and rules, which are measures used to determine the strength of the relationship between the items.\n",
    "\n",
    "Support is the percentage of transactions that contain the itemset, while confidence is the conditional probability of an item being purchased given that another item was already purchased. Lift measures the increase in the likelihood of purchasing an item given that another item has been purchased.\n",
    "\n",
    "MBA can be used to identify items that are frequently purchased together, which can be useful for product placement, cross-selling, and upselling strategies. For example, if MBA analysis reveals that customers who purchase diapers also tend to purchase beer, a retailer may choose to place beer near the diaper section to encourage customers to make an additional purchase.\n",
    "\n",
    "Overall, association analysis concepts are fundamental to MBA, which enables retailers to analyze transactional data and identify relationships between items that can be used to drive sales and improve customer satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db629ec-6215-4643-b8aa-19ea04aac2e0",
   "metadata": {},
   "source": [
    "3. Give an example of the Apriori algorithm for learning association rules.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bce919-823d-46e6-b861-4066e54d960d",
   "metadata": {},
   "source": [
    "Suppose we have a dataset of transactions from a grocery store, where each transaction consists of a set of items purchased by a customer. The items in the dataset include: bread, milk, eggs, cheese, yogurt, apples, and bananas.\n",
    "\n",
    "The Apriori algorithm can be used to learn association rules from this dataset. Let's say we want to find the rules that describe which items tend to be purchased together. We might start with a minimum support threshold of 50%, meaning that we want to find itemsets (sets of items) that appear in at least 50% of the transactions.\n",
    "\n",
    "First, we would scan the dataset to find all itemsets of size 1 (i.e., individual items) that meet the minimum support threshold. In this case, the itemsets that meet the threshold are: bread, milk, eggs, cheese, yogurt, apples, and bananas.\n",
    "\n",
    "Next, we would use these frequent itemsets to generate candidate itemsets of size 2. For example, we might generate the candidate itemset {bread, milk}. We would then scan the dataset again to find the support (i.e., frequency of occurrence) of each candidate itemset. If the support of a candidate itemset meets the minimum support threshold, we add it to the set of frequent itemsets of size 2.\n",
    "\n",
    "We repeat this process, generating candidate itemsets of size 3, 4, and so on, until we have found all frequent itemsets up to a given size.\n",
    "\n",
    "Once we have identified all frequent itemsets, we can use them to generate association rules. For example, we might generate the rule {bread, milk} => {eggs}, which would mean that customers who buy bread and milk are likely to also buy eggs. We can calculate the confidence of each rule (i.e., the proportion of transactions containing the antecedent that also contain the consequent) and filter out rules that do not meet a minimum confidence threshold.\n",
    "\n",
    "By using the Apriori algorithm to learn association rules, we can gain insights into the patterns of customer behavior and use this information to inform marketing and product placement strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeafd04-285a-41f4-a059-d4d3d1df1a42",
   "metadata": {},
   "source": [
    "4. In hierarchical clustering, how is the distance between clusters measured? Explain how this metric\n",
    "is used to decide when to end the iteration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e08201-55a0-49e0-b1ab-23d69af13a26",
   "metadata": {},
   "source": [
    "In hierarchical clustering, the distance between two clusters is measured using various distance metrics such as Euclidean distance, Manhattan distance, cosine distance, etc. These metrics calculate the distance between the centroid of one cluster and the centroid of another cluster or the distance between two closest points in each cluster. \n",
    "\n",
    "The iteration is stopped based on a stopping criterion. There are two commonly used stopping criteria in hierarchical clustering:\n",
    "\n",
    "1. Distance-based criterion: In this criterion, a threshold distance value is chosen. If the distance between two clusters is greater than the threshold value, then they are not merged, and the iteration stops. \n",
    "\n",
    "2. Number of clusters criterion: In this criterion, the iteration stops when a predefined number of clusters is reached. For example, if we want to create 3 clusters, the iteration stops when we have 3 clusters.\n",
    "\n",
    "The choice of stopping criterion depends on the problem and the data. In general, a distance-based criterion is preferred when the data is well separated, and a number-of-clusters-based criterion is preferred when the data is more dense or uniform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cee0898-a8c2-4a6b-84b7-d18aec33a3bd",
   "metadata": {},
   "source": [
    "5. In the k-means algorithm, how do you recompute the cluster centroids?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76537e87-0bc8-4da0-9719-a62f4bb49fff",
   "metadata": {},
   "source": [
    "In the k-means algorithm, after assigning the data points to their closest cluster centroid, the cluster centroids are recomputed by taking the mean of all the data points in each cluster. This mean represents the new centroid of the cluster. Mathematically, for a cluster j, the new centroid is computed as:\n",
    "\n",
    "centroid_j = (1 / N_j) * sum(x_i)\n",
    "\n",
    "where N_j is the number of data points in cluster j, sum(x_i) is the sum of all the data points in cluster j, and centroid_j is the new centroid of cluster j. This process is repeated until the centroids converge or a stopping criterion is met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8c259e-c288-41b8-90cc-2a5dab791565",
   "metadata": {},
   "source": [
    "6. At the start of the clustering exercise, discuss one method for determining the required number of\n",
    "clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16370ad1-3a17-4c0d-b19b-fe39f96b3fb7",
   "metadata": {},
   "source": [
    "One method for determining the required number of clusters is the elbow method. In this method, the sum of squared distances (SSE) of data points to their assigned cluster centroid is calculated for different values of k (the number of clusters). The SSE is plotted against the values of k, and the plot resembles an arm, hence the name \"elbow\" method. The point at which the elbow occurs is considered the optimal number of clusters. This point represents the tradeoff between minimizing the distance between data points and their assigned cluster centroid (which decreases with an increase in k) and minimizing the number of clusters (which also decreases with an increase in k). The elbow method can provide a quick and intuitive way to determine the number of clusters in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e3fa32-ede0-4c7a-a7df-54a7e70314df",
   "metadata": {},
   "source": [
    "7. Discuss the k-means algorithm's advantages and disadvantages.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a296cfb0-9950-4a47-b92d-b7c572221a25",
   "metadata": {},
   "source": [
    "Advantages of k-means algorithm:\n",
    "1. Simple and easy to implement.\n",
    "2. Fast and efficient for clustering large datasets.\n",
    "3. Can handle high dimensional data efficiently.\n",
    "4. Often produces tighter clusters than hierarchical clustering.\n",
    "\n",
    "Disadvantages of k-means algorithm:\n",
    "1. The number of clusters k must be known beforehand, which may not be feasible in real-world applications.\n",
    "2. Sensitive to initial cluster centroids and can converge to sub-optimal solutions.\n",
    "3. May not work well with clusters of different sizes, densities, or shapes.\n",
    "4. Cannot handle noise or outliers in the data.\n",
    "5. Assumes that all clusters are spherical and have the same variance, which may not be true in all cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c7d87c-0d25-4c6f-8a41-c6ebe14b395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Draw a diagram to demonstrate the principle of clustering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afddf345-fd0b-4b69-8828-97490907272c",
   "metadata": {},
   "source": [
    "I'm sorry, but as an AI language model, I don't have the ability to draw diagrams. However, I can describe the principle of clustering:\n",
    "\n",
    "The principle of clustering involves grouping together objects or data points that are similar to each other, while at the same time, keeping objects that are dissimilar to each other separate. This is typically done by defining a distance metric between the objects or data points and using an algorithm to iteratively group them based on their similarity.\n",
    "\n",
    "The result of clustering is a set of clusters, each containing objects or data points that are similar to each other and dissimilar to objects or data points in other clusters. This can be used for a variety of purposes, such as discovering patterns in data, identifying groups of similar objects, or making predictions based on similar data points.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eda9617-81f8-4f48-a3b3-9f8e8989fd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. During your study, you discovered seven findings, which are listed in the data points below. Using\n",
    "the K-means algorithm, you want to build three clusters from these observations. The clusters C1,\n",
    "C2, and C3 have the following findings after the first iteration:\n",
    "\n",
    "C1: (2,2), (4,4), (6,6); C2: (2,2), (4,4), (6,6); C3: (2,2), (4,4),\n",
    "\n",
    "C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,\n",
    "\n",
    "C3: (5,5) and (9,9)\n",
    "\n",
    "What would the cluster centroids be if you were to run a second iteration? What would this\n",
    "clustering's SSE be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef75711-c7bd-4837-b406-e3818d5d7b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b75af6a-146e-4f05-a102-f19c71092ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b3b5e0-bc6d-4a36-957d-61a00338ca36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6759df64-4078-4d49-8e10-4754789b2b44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
